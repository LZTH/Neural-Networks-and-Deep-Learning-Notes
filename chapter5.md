# 第五章 深度神经网络

这一章学习训练多层神经网络。

![](https://ooo.0o0.ooo/2015/11/22/5652861672265.png)


先说结论：如果还是使用随机梯度下降和反向传播算法来学习模型，效果会不好；不同层的神经元学习速度差别很大；如果层数大的神经元学习效果很好，则低层数的神经元基本上啥都学不到；如果低层数的神经元学习效果好，则高层数的神经元学习效果很差。

实际上，这是由于梯度下降算法本身不稳定造成的，这种不稳定导致对深度、多层神经网络训练时，要么低层数要么高层数的神经元学习效果很差。





## 梯度消失

前面作者直接给出了结论：梯度下降算法具有不稳定性，训练深度神经网络效果不好。

下面作者通过代码验证了这个结论，

首先，使用一层隐含层的网络来分类MNIST,验证集上分类准确率是96.48%，

然后，增加一层隐含层，其他参数不变，分类准确率提高到96.90%，

然后，再增加一层隐含层，其他参数不变，分类准确率不升反降，96.57%，

再增加一层隐含层，准确率又下降了，96.53%。四层隐含层效果基本和一层隐含层效果一样。

问题来了，层数越多，模型越复杂，分类效果应该更好才对啊。



我们不禁要问，对深度神经网络训练时，到底问题出在哪？

超参数的设定应该没问题，难道问题出在了学习算法上：梯度下降算法？（前面已经说过是梯度下降的问题了，这里还是故作吃惊状好了）


不失一般性，令$$\delta_{j}^{l} =\partial C/\partial b_{j}^{l}$$来表示第l层隐含层第j个神经元的学习速率，||$$\delta^{l}$$||表示第l层隐含层的参数学习速率，这里的参数学习速率指的是参数更新速度。

当神经网络只有两层隐含层时，我们画出每层隐含层的参数学习速率，

![](https://ooo.0o0.ooo/2015/11/23/5652b7ea73d8c.png)

可以观察到，layer 2比layer 1学习速率快，这是不是个别现象呢？

增加一层隐含层，试试，

![](https://ooo.0o0.ooo/2015/11/23/5652b84547430.png)

继续增加一层隐含层，


![](https://ooo.0o0.ooo/2015/11/23/5652b8810b580.png)

可以看到：对于同一个神经网络，l越小，神经元学习速率越慢；随着神经网络层数的增大，低层数神经元的学习速率在减慢！可以想象如果一个神经网络十层以上，前几层的神经元参数更得多慢啊！


这个现象称作 消失的梯度(vanishing gradient problem)。

梯度消失并不存在于每一个深度神经网络中，。。。。。。。。。。。
不要高兴的的太早，如果一个深度神经网络不存在题都消失问题，那它肯定存在另一个问题：梯度爆炸(exploding gradient problem),低层数的神经元参数更新过快！

对于深度神经网络，如果用基于梯度的算法来学习，就要格外注意梯度下降的不稳定性造成的梯度消失/爆炸问题。

## 梯度消失的起因：深度网络中梯度的不稳定性

梯度消失不是一个好现象，为了解决这个问题，我们先来好好认识一下它。

这是一个每层只有一个神经元的5层神经网络。

![](https://ooo.0o0.ooo/2015/11/23/5652bb8c41425.png)

我们可以直接写出C的表达式，然后链式法则算出$$\partial C/\partial b_{1}=\sigma '(z_{1})*w_{2}*\sigma'(z_{2})*w_{3}*\sigma'(z_{3})*w_{4}*\sigma'(z_{4})*\frac{\partial C}{\partial a_{4}}$$

作者是从左往右，一步一步算，
对于$$b_{1}$$,我们做一个$$\Delta b$$的改动，经过蝴蝶效应般的传播后，整个网络的输出都相应的改变了，我们做如下近似：



![](https://ooo.0o0.ooo/2015/11/23/5652be7cc9a02.png)

仔细分析蝴蝶效应的每一步：
$$a_{1}=\sigma(z_{1})=\sigma(w_{1}a_{0}+b_{1})$$,

![](https://ooo.0o0.ooo/2015/11/23/5652befed6c62.png)

$$\Delta a_{1}$$又引起了$$z_{2}$$的改变，

![](https://ooo.0o0.ooo/2015/11/23/5652c044c3978.png)


继续这个步骤，。。。。，最终得到：

![](https://ooo.0o0.ooo/2015/11/23/5652c08d6e562.png)


等号两边同除以$$\Delta b_{1}$$,得到

![](https://ooo.0o0.ooo/2015/11/23/5652c12628d21.png)

注意到，$$\partial C/\partial b_{1}$$基本上可以看作$$w_{j}\sigma'(z_{j})$$的联乘。


回忆 $$\sigma'$$的函数图像：
![](https://ooo.0o0.ooo/2015/11/23/5652c183ba85e.png)

最大值为0.25，我们使用N(0,1)对网络的参数进行初始化时，大部分的$$|w_{j}|<1$$,所以通常$$|w_{j}\sigma'(z_{j})|<0.25$$,这就导致$$\partial C/b_{l}$$近似指数级下降。

//而当使用反向传播从右往左计算时，通常$$|w_{j}\sigma'(z_{j})|>1$$,又引起了梯度爆炸问题。



不论是梯度消失还是梯度爆炸，根本原因是 求低层数神经元的偏导数时，要将所有高层数神经元中的某些项做联乘。

当神经网络层数很大时，这就必然引起问题。



Note：如果使用sigmoid神经元，更容易引起梯度消失，因为$$\sigma'(z)=\sigma'(wa+b)$$,如果w很大，$$\sigma'(z)$$必然很小，导致$$|w\sigma'(z)|>1不是那么容易发生$$。

不记得是谁说过，“当你系统的数据量提高一个数量级后，就该考虑重构了”。

前三章的神经网络学起来似乎并不困难，那是因为我们碰到的只是toy network，一旦面对复杂的深度神经网络，连原本看似好用的梯度下降算法都遇到了难题，任重而道远啊。









