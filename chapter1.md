# 使用神经网络进行手写数字识别



---


人类的视觉系统简直神奇，看下面一幅图，很容易猜到图片里面的数字吧？
![](https://ooo.0o0.ooo/2015/11/09/5640a35dbab7d.png)

当如果向通过写程序让计算机也掌握这种能力，可就一点也不简单了，比如果数字 '9'，想一想我们是怎么认出她的，上半部分一个圆圈 下半部分一条竖线。可是怎样用代码/算法 告诉计算机呢？It's difficult.

神经网络从另一条思路来解决这个问题：输入很多手写数字样本(训练集)，然后训练出一个模型，这个模型具有识别手写数字的能力。

![](https://ooo.0o0.ooo/2015/11/09/5640a60ee9479.png)


换句话说，我们不需要告诉神经网络用什么样子的算法/规则来描述区分0,1,2,3...9.她自己会自动推算出这些规则。 excited! 并且随着训练集中样本的增加，模型的能力也越强哦。excited again!



本章我们就要着手撸一个naive滴神经网络来识别图片中的数字。
仅仅74代码还不需要调用第三方神经网络库，却拥有超过96%的识别准确率！

本章除了写代码，还要讲解不少重要的概念，比如两类重要的神经元(感知机和sigmoi神经元)，神经网络参数学习算法(随机梯度下降)。

为了加深印象，每次在讲HOW之前，我们都会先谈一谈WHY。

##感知机

什么是神经网络？在了解更深入的知识前，先介绍一类人工神经元：感知机。

上世纪50年代Dr. Frank Rosenblatt就提出了感知机的概念。虽然如今sigmoi神经元更广为使用，我们还是有必要先介绍感知机。



感知机接收一些二元变量，然后输出一个二元变量。

![](https://ooo.0o0.ooo/2015/11/09/5640aa72784e1.png)

上图的感知机模型有三个输入{$$x_{1},x_{2},x_{3}$$}，一个输出。
怎样计算输出值呢？ Rosenblatt提出了一个简单的算法。他引入了新的实数值变量：$$w$$。用于表示相对于输出变量每个输入变量的重要性(权重)。

![](https://ooo.0o0.ooo/2015/11/09/5640ac813f0ef.png)


通过变化$$w$$和$$threshold$$ 我们就得到了不同的感知机模型。

一个感知机单元看似简单，但是如果多个感知机组合成复杂的多层神经网络，能力还是很强大的，比如这种：
![](https://ooo.0o0.ooo/2015/11/09/5640adc6cb5b2.png)

上图中相比于第一层的感知机单元，第二层的感知机能做出更抽象更复杂的决策。

注意：一个感知机仅有一个输出。上图中第一层感知机的输出一侧有三条线，这仅仅表示将感知机的输出作为第二层每一个感知机的输入而已。

下面给出一种更加简化的感知机描述形式。

![](https://ooo.0o0.ooo/2015/11/09/564158aa4058c.png)

简单说明一下：$$w\cdot x\equiv \sum_{j}w_{j}x_{j}$$,此处$$w$$和$$x$$均为向量; b$$\equiv -$$threshold,此处b被称作感知机的偏置(bias)，你可以将偏置理解为感知机输出1的难易程度，如果b是非常大的正数，显然对于此时的感知机输出1非常容易，反之，如果b是一个非常小的负数，感知机很难输出1，正因为引入了偏置的概念使得我们的描述更加方便且容易理解，以后我们将不再使用threshold这个概念，统一用偏置代替。

感知机不仅可以用于做出决策，还可以用来表示逻辑电路中基本的门电路，例如与门(AND), 或门(OR), 与非门(NAND)。

举个例子，

![](https://ooo.0o0.ooo/2015/11/09/56415c11139a1.png)

上图中的感知机有两个输入变量，权重均为-2。如果令$$x_{1},x_{2}$$取值为0，则$$(-2)*0+(-2)*0+3=3$$,因此感知机输出1；如果令$$x_{1},x_{2}$$取值为1，则$$(-2)*1+(-2)*1+3=-1$$,因此感知机输出0;如果令$$x_{1},x_{2}$$一个取值0，另一个取值1，则感知机输出1.

看到没，输入00，输出1；输入11，输出0；输入01/10，输出1，这不就是与非门吗！

更加excited的还在后面：我们可以使用感知机组成的神经网络表示任意的一个逻辑电路！
这个感知机的优点同时也是她的缺点：既然感知机组成的神经网络可以等价任意逻辑电路，我们直接使用逻辑电路不就得了，干嘛还非得用感知机组成神经网络呢？

##Sigmoid神经元

对于一个神经网络而言，什么是学习？我们可以认为学习就是给定输入，不断的调整各个权重和偏置，以使得神经网络的输出就是我们想要的结果。这就要求神经网络具有一种性质：改变某一个权重或偏置很小的值，整个神经网络的输出也应该改变很小。否则这种学习就会非常困难。

![](https://ooo.0o0.ooo/2015/11/09/564163b8ae6a7.png)

不幸的是，感知机组成的神经网络就不具有这种性质。很可能你只是靴微改变某一个权重的值，整个神经网络的输出却会发生质变：原来输出0，现在输出1.

碰到这个问题怎么办呢？前人因此引入了一种新的神经元类型：Sigmoid神经元。

![](https://ooo.0o0.ooo/2015/11/09/56416626a87fc.png)

同样，$$x_{1},x_{2},x_{3}$$是输入变量，也含有权重$$w$$和偏置b，不同于感知机的是，输入变量不仅可以取值0或1，还可以取值0和1之间的任何实数！输出值也不局限于0或1，而是sigmoid函数$$\sigma (w\cdot x+b)$$,

![](https://ooo.0o0.ooo/2015/11/09/564167847f36f.png)

(注意,$$\sigma 也被称作逻辑斯蒂(logistic)函数，所以有很多资料也把这里的sigmoid神经元称作逻辑斯蒂神经元$$.)

因此，sigmoid神经元的输出值：

![](https://ooo.0o0.ooo/2015/11/09/564167bfce2a0.png)

第一眼看上去，感觉sigmoid神经元和感知机差别好大。实际上，她俩还是很相似的，不妨假设$$z\equiv w\cdot x+b$$是一个非常大的正数，则可以得到$$e^{-z}\approx 0 \Rightarrow \sigma (z)\approx 1$$。反之，假设$$z\equiv w\cdot x+b$$是一个非常小的负数，则可以得到$$e^{-z}\to +\infty, \sigma (z)\approx 0$$。当然啦，如果$$z$$的取值大小适中，此时sigmoid神经元和感知机的差别就体现出来了。

对于sigmoid函数，我们都需要掌握她哪些性质呢？实际上，你只需要知道她的形状就可以了。

![](https://ooo.0o0.ooo/2015/11/09/56416b2cbd24d.png)

sigmoid函数其实是下图阶梯函数的平滑版本。

![](https://ooo.0o0.ooo/2015/11/09/56416b7eba8e4.png)

其实，你可以把上图的阶梯函数理解为感知机。现在根据这两幅图，是不是很好理解感知机和sigmoid神经元的异同了:))

正是因为sigmoid函数这种平滑特性，使得改变某个$$w$$或$$b$$很小的值，输出也仅仅改变很小。

由sigmoid函数易知，$$\sigma$$对$$(w,b)$$可微分，得下式：

![](https://ooo.0o0.ooo/2015/11/09/56416cbe6c66d.png)

如果你忘记了可微的概念，没关系，你只需要知道上式表明：$$\Delta output$$是$$\Delta w_{i}$$和$$\Delta b$$
的*线性函数*。这就使得靴微改变$$w$$和$$b$$的值来慢慢调整输出值变得可能。
sigmoid函数被广泛当作激活函数还有另一原因，她含有幂函数，幂函数在求导过程中的不变性可以极大简化(5)式中求偏导的计算。


#神经网络的结构

假设现有以下神经网络：

![](https://ooo.0o0.ooo/2015/11/10/56419303036f8.png)

让我们再回顾一下基本知识：上图最左侧一列是输入层，中间一列是隐藏层，最右侧一列是输出层。

