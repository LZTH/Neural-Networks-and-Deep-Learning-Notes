# 使用神经网络进行手写数字识别



---


人类的视觉系统简直神奇，看下面一幅图，很容易猜到图片里面的数字吧？
![](https://ooo.0o0.ooo/2015/11/09/5640a35dbab7d.png)

当如果向通过写程序让计算机也掌握这种能力，可就一点也不简单了，比如果数字 '9'，想一想我们是怎么认出她的，上半部分一个圆圈 下半部分一条竖线。可是怎样用代码/算法 告诉计算机呢？It's difficult.

神经网络从另一条思路来解决这个问题：输入很多手写数字样本(训练集)，然后训练出一个模型，这个模型具有识别手写数字的能力。

![](https://ooo.0o0.ooo/2015/11/09/5640a60ee9479.png)


换句话说，我们不需要告诉神经网络用什么样子的算法/规则来描述区分0,1,2,3...9.她自己会自动推算出这些规则。 excited! 并且随着训练集中样本的增加，模型的能力也越强哦。excited again!



本章我们就要着手撸一个naive滴神经网络来识别图片中的数字。
仅仅74代码还不需要调用第三方神经网络库，却拥有超过96%的识别准确率！

本章除了写代码，还要讲解不少重要的概念，比如两类重要的神经元(感知机和sigmoi神经元)，神经网络参数学习算法(随机梯度下降)。

为了加深印象，每次在讲HOW之前，我们都会先谈一谈WHY。

##感知机

什么是神经网络？在了解更深入的知识前，先介绍一类人工神经元：感知机。

上世纪50年代Dr. Frank Rosenblatt就提出了感知机的概念。虽然如今sigmoi神经元更广为使用，我们还是有必要先介绍感知机。



感知机接收一些二元变量，然后输出一个二元变量。

![](https://ooo.0o0.ooo/2015/11/09/5640aa72784e1.png)

上图的感知机模型有三个输入{$$x_{1},x_{2},x_{3}$$}，一个输出。
怎样计算输出值呢？ Rosenblatt提出了一个简单的算法。他引入了新的实数值变量：$$w$$。用于表示相对于输出变量每个输入变量的重要性(权重)。

![](https://ooo.0o0.ooo/2015/11/09/5640ac813f0ef.png)


通过变化$$w$$和$$threshold$$ 我们就得到了不同的感知机模型。

一个感知机单元看似简单，但是如果多个感知机组合成复杂的多层神经网络，能力还是很强大的，比如这种：
![](https://ooo.0o0.ooo/2015/11/09/5640adc6cb5b2.png)

上图中相比于第一层的感知机单元，第二层的感知机能做出更抽象更复杂的决策。

注意：一个感知机仅有一个输出。上图中第一层感知机的输出一侧有三条线，这仅仅表示将感知机的输出作为第二层每一个感知机的输入而已。

下面给出一种更加简化的感知机描述形式。

![](https://ooo.0o0.ooo/2015/11/09/564158aa4058c.png)

简单说明一下：$$w\cdot x\equiv \sum_{j}w_{j}x_{j}$$,此处$$w$$和$$x$$均为向量; b$$\equiv -$$threshold,此处b被称作感知机的偏置(bias)，你可以将偏置理解为感知机输出1的难易程度，如果b是非常大的正数，显然对于此时的感知机输出1非常容易，反之，如果b是一个非常小的负数，感知机很难输出1，正因为引入了偏置的概念使得我们的描述更加方便且容易理解，以后我们将不再使用threshold这个概念，统一用偏置代替。

感知机不仅可以用于做出决策，还可以用来表示逻辑电路中基本的门电路，例如与门(AND), 或门(OR), 与非门(NAND)。

举个例子，

![](https://ooo.0o0.ooo/2015/11/09/56415c11139a1.png)

上图中的感知机有两个输入变量，权重均为-2。如果令$$x_{1},x_{2}$$取值为0，则$$(-2)*0+(-2)*0+3=3$$,因此感知机输出1；如果令$$x_{1},x_{2}$$取值为1，则$$(-2)*1+(-2)*1+3=-1$$,因此感知机输出0;如果令$$x_{1},x_{2}$$一个取值0，另一个取值1，则感知机输出1.

看到没，输入00，输出1；输入11，输出0；输入01/10，输出1，这不就是与非门吗！

更加excited的还在后面：我们可以使用感知机组成的神经网络表示任意的一个逻辑电路！
这个感知机的优点同时也是她的缺点：既然感知机组成的神经网络可以等价任意逻辑电路，我们直接使用逻辑电路不就得了，干嘛还非得用感知机组成神经网络呢？

##Sigmoid神经元

对于一个神经网络而言，什么是学习？我们可以认为学习就是给定输入，不断的调整各个权重和偏置，以使得神经网络的输出就是我们想要的结果。这就要求神经网络具有一种性质：改变某一个权重或偏置很小的值，整个神经网络的输出也应该改变很小。否则这种学习就会非常困难。

![](https://ooo.0o0.ooo/2015/11/09/564163b8ae6a7.png)

不幸的是，感知机组成的神经网络就不具有这种性质。很可能你只是靴微改变某一个权重的值，整个神经网络的输出却会发生质变：原来输出0，现在输出1.

碰到这个问题怎么办呢？前人因此引入了一种新的神经元类型：Sigmoid神经元。

![](https://ooo.0o0.ooo/2015/11/09/56416626a87fc.png)

同样，$$x_{1},x_{2},x_{3}$$是输入变量，也含有权重$$w$$和偏置b，不同于感知机的是，输入变量不仅可以取值0或1，还可以取值0和1之间的任何实数！输出值也不局限于0或1，而是sigmoid函数$$\sigma (w\cdot x+b)$$,

![](https://ooo.0o0.ooo/2015/11/09/564167847f36f.png)

(注意,$$\sigma 也被称作逻辑斯蒂(logistic)函数，所以有很多资料也把这里的sigmoid神经元称作逻辑斯蒂神经元$$.)

因此，sigmoid神经元的输出值：

![](https://ooo.0o0.ooo/2015/11/09/564167bfce2a0.png)

第一眼看上去，感觉sigmoid神经元和感知机差别好大。实际上，她俩还是很相似的，不妨假设$$z\equiv w\cdot x+b$$是一个非常大的正数，则可以得到$$e^{-z}\approx 0 \Rightarrow \sigma (z)\approx 1$$。反之，假设$$z\equiv w\cdot x+b$$是一个非常小的负数，则可以得到$$e^{-z}\to +\infty, \sigma (z)\approx 0$$。当然啦，如果$$z$$的取值大小适中，此时sigmoid神经元和感知机的差别就体现出来了。

对于sigmoid函数，我们都需要掌握她哪些性质呢？实际上，你只需要知道她的形状就可以了。

![](https://ooo.0o0.ooo/2015/11/09/56416b2cbd24d.png)

sigmoid函数其实是下图阶梯函数的平滑版本。

![](https://ooo.0o0.ooo/2015/11/09/56416b7eba8e4.png)

其实，你可以把上图的阶梯函数理解为感知机。现在根据这两幅图，是不是很好理解感知机和sigmoid神经元的异同了:))

正是因为sigmoid函数这种平滑特性，使得改变某个$$w$$或$$b$$很小的值，输出也仅仅改变很小。

由sigmoid函数易知，$$\sigma$$对$$(w,b)$$可微分，得下式：

![](https://ooo.0o0.ooo/2015/11/09/56416cbe6c66d.png)

如果你忘记了可微的概念，没关系，你只需要知道上式表明：$$\Delta output$$是$$\Delta w_{i}$$和$$\Delta b$$
的*线性函数*。这就使得靴微改变$$w$$和$$b$$的值来慢慢调整输出值变得可能。
sigmoid函数被广泛当作激活函数还有另一原因，她含有幂函数，幂函数在求导过程中的不变性可以极大简化(5)式中求偏导的计算。


#神经网络的结构

假设现有以下神经网络：

![](https://ooo.0o0.ooo/2015/11/10/56419303036f8.png)

让我们再回顾一下基本知识：上图最左侧一列是输入层(input layer)，中间一列是隐含层(hidden layer)，最右侧一列是输出层(output layer)。隐含层这个名字仅仅表示她不是输入层也不是输出层，并且隐含层可以不止一个：

![](https://ooo.0o0.ooo/2015/11/10/56419413aa6d7.png)

*注意*：由于历史原因，多层的神经网络有时也被称作多层感知机(multilayer perceptrons)/MLP，即使这个神经网络是由sigmoid神经元组成的！为了不至于混淆，本书不适用MLP这个术语。


输入层和输出层的设计通常是很简单的，假设我们要让神经网络判断图片中的数字是否是'9'，图片是64\*64的灰度图像，我们可以直接让每个神经元接受一个像素单元，即需要64*64=4096个输入神经元，每个输入神经元取值范围是[0,1]。输出层含有一个神经元即可，如果输出值小于0.5表示”输入图片不是9“，否则”输入图片数字是9“。


相比较输入输出层的简单粗暴，隐含层的设计就堪称是一门艺术了。当然不要担心，前人已经总结了许多启发式设计方法。这些启发式方法可以用于帮助我们在设计隐含层层数和神经网络训练时间之间找到一种折衷方案。



到目前为止，我们举例的神经网络都有一个共性：从左向右，从输入层到输出层，每个神经元接受前一级的输入，并输出到下一级。这一大类神经网络也被称作前向(feedforward)神经网络。此类神经网络不包含回路，信息只能自输入层方向流向输出层方向，而不能走回头路。本书的内容只限于讲解前向神经网络，RNN等神经网络暂不涉及。


#一个简单的手写数字分类器

到此为止，神经网络的最基本知识已经讲完了，让我们回到最初要解决的问题：识别手写数字。
我们可以将这个问题分为两个子问题，首先将一幅含有多个数字的图像分割成一组只含有一个数字的图像。
比如，对于下图

![](https://ooo.0o0.ooo/2015/11/10/56419b4e56b12.png)

我们想要得到六幅图像

![](https://ooo.0o0.ooo/2015/11/10/56419b6bd11e6.png)

一旦第一步完成，程序需要将每一幅图像进行分类处理。即能够识别5 0 4 1 9 2 

对于第一步的处理不是我们要讲的重点，我们着重讲解第二步怎么做。

为了完成手写数字识别任务，我们将要使用一个三层神经网络：

![](https://ooo.0o0.ooo/2015/11/10/56419cc400c90.png)

我们规定输入层是一幅28*28的灰度图像，因此输入神经元有784个，上图并没有把整个神经网络画出来，每个输入神经元取值[0,1]。

隐含层中神经元的个数难以确定，因此我们用变量$$n$$表示，$$n$$可以取不同的值。暂时令$$n=15$$。

输出层含有10个神经元，我们通过查看这10个神经元的输出，含有最大输出值的神经元对应的数字即为整个神经网络的输出。

有人可能会想，既然每个神经元最终的输出是0或1，4个神经元就可以表示16种状态，为什么还要用10个神经元呢？只能说，这是经验，经验表明对于手写数字识别这个问题，含有10个神经元的输出层要比含有4个神经元的效果好。这个答案显然不能令人满意。

作者给出了一种启发式想法：对于当前的分类器，她的任务是识别0-9共10个数字，最直观的做法就是输出层含有10个神经元，每个神经元分别识别相对应的数字就可以了，比如，第一个神经元$$o_{1}$$的作用是专门识别图像数字是否是0，假设隐含层含有4个神经元，$$o_{1}$$要根据隐含层中的四部分输出决定图像是否为0，更进一步，我们可以这样假设：每一个数字都可以拆成四部分，隐含层每个神经元分别判断自己负责的那部分图像，比如0可以拆成

![](https://ooo.0o0.ooo/2015/11/10/5641ded88def8.png)
![](https://ooo.0o0.ooo/2015/11/10/5641deef07626.png)

$$h_{1}$$只判断图像右上角区域是否是半条弧线，然后给出一个概率值，输出层的神经元根据隐含层提供的概率值给出最终的决策。这么想好像挺合理，但是具体神经网络是不是这样做的，可不知道。


#梯度下降

上一节中给出了神经网络的结构，这一节将要学习怎么”学习参数“。

数据集我们使用[MNIST](http://yann.lecun.com/exdb/mnist/)，里面含有很多手写数字图像以及对应的标签，这里指图像中的数字。MNIST分为两部分：60000张28\*28的灰度图像是训练集，10000张28*28的灰度图像作为测试集。


我们使用$$(x,y)$$来表示一个训练样本，其中$$y=y(x)$$，$$x$$是784维度的向量，$$y$$是10维度向量形如$$(1,0,0,0,0,0,0,0,0,0)^T$$。

损失函数为：

![](https://ooo.0o0.ooo/2015/11/10/5641e403b07d9.png)

其中$$w$$表示网络中所有权重，$$b$$表示所有偏置，$$n$$表示训练样本个数，$$a$$表示神经网络输入$$x$$时的输出。

我们的训练目标就是找到合适的$$w,b$$使得$$C(w,b)\approx 0$$。并且可以看出$$C(w,b)>=0$$，套用数学术语就是，对于函数$$C(w,b)$$我们要确定合适的$$w,b$$使函数值最小。使用的算法就是梯度下降。

在讲梯度下降之前，先回顾下高数知识:(

![](https://ooo.0o0.ooo/2015/11/10/5641e8fe30935.png)

怎么求出函数$$C(v_{1},v_{2})$$的最小值？一般的做法是先求出偏导数，然后令偏导为0，得到驻点，然后确定出最小值。

神经网络也能这样做吗？一张纸，一支笔，一个计算器，能不能确定出参数，得到最小值？显然不靠谱，一个简单的神经网络参数至少几万个并且函数形式异常复杂，想要算出解析解不可能。

既然不能从全局考虑计算最小值，能否从局部角度来处理呢？为了形象化这个问题，不妨把上图中的曲面当作一个峡谷，小明站在谷顶，我们的目的是让他走到谷底。


![](https://ooo.0o0.ooo/2015/11/10/5641ec00a6aff.png)



再次回顾高数学到的知识:梯度。在二元函数情形，设二元函数$$f(x,y)$$在平面区域D内具有一阶连续偏导数，则对于每一点$$P_{0}(x_{0},y_{0})\in D$$，都可定出一个向量

$$(f_{x}(x_{0},y_{0}), f_{y}(x_{0},y_{0}))^{T}$$,
这个向量称为函数$$f(x,y)$$在点$$P_{0}(x_{0},y_{0})$$的梯度，记作$$\mathbf{grad}f(x_{0},y_{0})$$或$$\nabla f(x_{0},y_{0})$$.

$$\nabla = (\frac{\partial}{\partial x} + \frac{\partial}{\partial y})^{T} $$被称为向量微分算子或Nabla算子，$$\nabla f= (\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y})^{T}$$。

如果二元函数$$f(x,y)$$在点$$P_{0}(x_{0},y_{0})$$可微分，$$\mathbf{e_{l}}=(cos\alpha,cos\beta)$$是与方向$$l$$同向的单位向量，则

$$\frac{\partial f}{\partial l}\rvert _{(x_{0},y_{0})} = f_{x}(x_{0},y_{0})cos\alpha + f_{y}(x_{0},y_{0})cos\beta=\mathbf{grad}f(x_{0},y_{0})\cdot \mathbf{e_{l}} = |\mathbf{grad}f(x_{0},y_{0})|cos\theta$$

因此，当$$\theta =0$$，即方向$$\mathbf{e_{l}}$$与梯度$$\mathbf{grad}f(x_{0},y_{0})$$方向相同，函数$$f(x,y)$$增加最快。

回到我们刚才的问题，怎样从局部角度考虑让小明一点一点的走到谷底呢？这其中的关键就是要确定小明每一步行走的方向和每一步的步长，使得小明一步步的靠近谷底。既然有了梯度的概念，该确定什么方向也就明了了：逆梯度方向。假设步长固定，小明每一次抬脚后都按照当前所在位置的逆梯度方向落脚，这种走法不但能走到谷底并且用时最短。这就是梯度下降算法，其中步长称为学习率。

在实际应用过程中梯度下降算法也会遇到不少问题，这里只提一个，在公式(6)中定义了我们设计的神经网络的损失函数


![](https://ooo.0o0.ooo/2015/11/10/5641e403b07d9.png)

上式可以写成$$C=\frac{1}{n} \sum_{x}C_{x}, 其中C_{x}=\frac{||y(x)-a||^{2}}{2}$$，损失函数的梯度$$\nabla C=\frac{1}{n}\sum_{x}\nabla C_{x}$$, 假设权重和偏置变量个数为$$M$$, 在每一次对损失函数求梯度过程中，实际要对每一个训练样本求梯度，而求梯度过程就是计算偏导数的过程，因此在每一点求梯度都要计算M*n次偏导数！这无疑是一个非常耗时的过程。

随机梯度下降算法(stochastic gradient descent)应运而生，SGD在每一次计算$$\nabla C$$时不使用整个训练集而是随机选取部分训练样本计算它们的$$\nabla C_{x}$$，用部分训练集的梯度平均值近似看作损失函数的梯度。



















































